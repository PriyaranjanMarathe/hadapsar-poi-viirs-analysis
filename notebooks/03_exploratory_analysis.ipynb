{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - POI-VIIRS Integration\n",
    "\n",
    "This notebook provides comprehensive exploratory data analysis of the integrated POI-VIIRS dataset, including statistical analysis, correlation studies, spatial patterns, and key insights.\n",
    "\n",
    "## Objectives\n",
    "1. Perform comprehensive statistical analysis\n",
    "2. Analyze correlations between POI categories and luminosity\n",
    "3. Study spatial patterns and autocorrelation\n",
    "4. Evaluate clustering results\n",
    "5. Investigate anomalous patterns\n",
    "6. Generate actionable insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Import custom modules\n",
    "from analysis.exploratory_analysis import ExploratoryAnalyzer\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Integrated Dataset\n",
    "\n",
    "Let's load the integrated POI-VIIRS dataset created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the exploratory analyzer\n",
    "analyzer = ExploratoryAnalyzer(data_dir=\"../data\", output_dir=\"../results\")\n",
    "\n",
    "print(\"Exploratory analyzer initialized\")\n",
    "print(f\"Data directory: {analyzer.data_dir}\")\n",
    "print(f\"Output directory: {analyzer.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the integrated dataset\n",
    "print(\"Loading integrated POI-VIIRS dataset...\")\n",
    "integrated_gdf = analyzer.load_integrated_data()\n",
    "\n",
    "print(f\"Dataset loaded: {len(integrated_gdf)} records\")\n",
    "print(f\"Columns: {list(integrated_gdf.columns)}\")\n",
    "print(f\"CRS: {integrated_gdf.crs}\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset shape: {integrated_gdf.shape}\")\n",
    "print(f\"Memory usage: {integrated_gdf.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Show first few records\n",
    "display(integrated_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Descriptive Statistics\n",
    "\n",
    "Let's start with comprehensive descriptive statistics of our integrated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate basic descriptive statistics\n",
    "print(\"Calculating basic descriptive statistics...\")\n",
    "desc_stats = analyzer.basic_descriptive_stats(integrated_gdf)\n",
    "\n",
    "print(\"\\nBASIC DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nDataset Overview:\")\n",
    "print(f\"  Total POIs: {desc_stats['total_pois']:,}\")\n",
    "print(f\"  Unique Categories: {desc_stats['unique_categories']}\")\n",
    "\n",
    "if 'luminosity_stats' in desc_stats:\n",
    "    print(f\"\\nLuminosity Statistics:\")\n",
    "    lum_stats = desc_stats['luminosity_stats']\n",
    "    print(f\"  Count: {lum_stats['count']:.0f}\")\n",
    "    print(f\"  Mean: {lum_stats['mean']:.3f}\")\n",
    "    print(f\"  Std Dev: {lum_stats['std']:.3f}\")\n",
    "    print(f\"  Min: {lum_stats['min']:.3f}\")\n",
    "    print(f\"  25%: {lum_stats['25%']:.3f}\")\n",
    "    print(f\"  50% (Median): {lum_stats['50%']:.3f}\")\n",
    "    print(f\"  75%: {lum_stats['75%']:.3f}\")\n",
    "    print(f\"  Max: {lum_stats['max']:.3f}\")\n",
    "\n",
    "print(f\"\\nCategory Distribution:\")\n",
    "total_pois = desc_stats['total_pois']\n",
    "for category, count in desc_stats['category_distribution'].items():\n",
    "    percentage = (count / total_pois) * 100\n",
    "    print(f\"  {category}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "if 'spatial_extent' in desc_stats:\n",
    "    extent = desc_stats['spatial_extent']\n",
    "    print(f\"\\nSpatial Extent:\")\n",
    "    print(f\"  Longitude: {extent['min_longitude']:.6f} to {extent['max_longitude']:.6f}\")\n",
    "    print(f\"  Latitude: {extent['min_latitude']:.6f} to {extent['max_latitude']:.6f}\")\n",
    "    print(f\"  Approximate area: {extent['width_km']:.1f} × {extent['height_km']:.1f} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed descriptive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Comprehensive Descriptive Statistics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Category distribution (pie chart)\n",
    "ax1 = axes[0, 0]\n",
    "category_counts = integrated_gdf['category_group'].value_counts()\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(category_counts)))\n",
    "wedges, texts, autotexts = ax1.pie(category_counts.values, labels=category_counts.index, \n",
    "                                  autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax1.set_title('POI Category Distribution')\n",
    "\n",
    "# 2. Luminosity distribution (histogram with KDE)\n",
    "ax2 = axes[0, 1]\n",
    "if 'viirs_luminosity' in integrated_gdf.columns:\n",
    "    ax2.hist(integrated_gdf['viirs_luminosity'], bins=40, alpha=0.7, \n",
    "             color='skyblue', edgecolor='black', density=True, label='Histogram')\n",
    "    # Add KDE curve\n",
    "    from scipy.stats import gaussian_kde\n",
    "    kde_data = integrated_gdf['viirs_luminosity'].dropna()\n",
    "    if len(kde_data) > 1:\n",
    "        kde = gaussian_kde(kde_data)\n",
    "        x_range = np.linspace(kde_data.min(), kde_data.max(), 100)\n",
    "        ax2.plot(x_range, kde(x_range), 'red', linewidth=2, label='KDE')\n",
    "    ax2.set_xlabel('VIIRS Luminosity')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.set_title('Luminosity Distribution')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Category vs Luminosity (boxplot)\n",
    "ax3 = axes[0, 2]\n",
    "if 'viirs_luminosity' in integrated_gdf.columns:\n",
    "    box_data = [integrated_gdf[integrated_gdf['category_group'] == cat]['viirs_luminosity'].values \n",
    "                for cat in category_counts.index]\n",
    "    bp = ax3.boxplot(box_data, labels=[cat[:10] + '...' if len(cat) > 10 else cat \n",
    "                                      for cat in category_counts.index], patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax3.set_ylabel('VIIRS Luminosity')\n",
    "    ax3.set_title('Luminosity by Category')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Spatial distribution\n",
    "ax4 = axes[1, 0]\n",
    "if 'viirs_luminosity' in integrated_gdf.columns:\n",
    "    scatter = ax4.scatter(integrated_gdf.geometry.x, integrated_gdf.geometry.y,\n",
    "                         c=integrated_gdf['viirs_luminosity'], s=20, alpha=0.7,\n",
    "                         cmap='viridis', edgecolors='white', linewidths=0.2)\n",
    "    plt.colorbar(scatter, ax=ax4, label='VIIRS Luminosity')\n",
    "else:\n",
    "    ax4.scatter(integrated_gdf.geometry.x, integrated_gdf.geometry.y, s=20, alpha=0.7)\n",
    "ax4.set_xlabel('Longitude')\n",
    "ax4.set_ylabel('Latitude')\n",
    "ax4.set_title('Spatial Distribution of POIs')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Cluster distribution (if available)\n",
    "ax5 = axes[1, 1]\n",
    "if 'cluster' in integrated_gdf.columns:\n",
    "    cluster_counts = integrated_gdf['cluster'].value_counts().sort_index()\n",
    "    colors_cluster = plt.cm.tab10(np.linspace(0, 1, len(cluster_counts)))\n",
    "    bars = ax5.bar(cluster_counts.index, cluster_counts.values, color=colors_cluster)\n",
    "    ax5.set_xlabel('Cluster ID')\n",
    "    ax5.set_ylabel('Number of POIs')\n",
    "    ax5.set_title('Cluster Size Distribution')\n",
    "    ax5.set_xticks(cluster_counts.index)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "else:\n",
    "    ax5.text(0.5, 0.5, 'No cluster data available', ha='center', va='center', \n",
    "             transform=ax5.transAxes, fontsize=12)\n",
    "    ax5.set_title('Cluster Analysis')\n",
    "\n",
    "# 6. Anomaly distribution (if available)\n",
    "ax6 = axes[1, 2]\n",
    "if 'anomaly_type' in integrated_gdf.columns:\n",
    "    anomaly_counts = integrated_gdf['anomaly_type'].value_counts()\n",
    "    colors_anomaly = ['#28a745', '#ffc107', '#dc3545', '#6c757d'][:len(anomaly_counts)]\n",
    "    wedges, texts, autotexts = ax6.pie(anomaly_counts.values, labels=anomaly_counts.index,\n",
    "                                      autopct='%1.1f%%', colors=colors_anomaly, startangle=90)\n",
    "    ax6.set_title('Anomaly Type Distribution')\n",
    "else:\n",
    "    ax6.text(0.5, 0.5, 'No anomaly data available', ha='center', va='center',\n",
    "             transform=ax6.transAxes, fontsize=12)\n",
    "    ax6.set_title('Anomaly Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis\n",
    "\n",
    "Let's analyze the correlations between POI categories and luminosity patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform correlation analysis\n",
    "print(\"Performing POI-luminosity correlation analysis...\")\n",
    "correlation_results = analyzer.poi_luminosity_correlation_analysis(integrated_gdf)\n",
    "\n",
    "print(\"\\nCORRELATION ANALYSIS RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display category-luminosity correlations\n",
    "if 'category_luminosity_correlations' in correlation_results:\n",
    "    print(\"\\nCategory-Luminosity Correlations:\")\n",
    "    correlations = correlation_results['category_luminosity_correlations']\n",
    "    \n",
    "    # Sort by absolute correlation value\n",
    "    sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    for category, correlation in sorted_correlations:\n",
    "        strength = \"Strong\" if abs(correlation) > 0.5 else \"Moderate\" if abs(correlation) > 0.3 else \"Weak\"\n",
    "        direction = \"Positive\" if correlation > 0 else \"Negative\"\n",
    "        print(f\"  {category}: {correlation:.3f} ({strength} {direction})\")\n",
    "\n",
    "# Display statistical significance tests\n",
    "if 'correlation_tests' in correlation_results:\n",
    "    print(\"\\nStatistical Significance Tests (Mann-Whitney U):\")\n",
    "    print(\"Category vs Others - Luminosity Comparison\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for category, test_results in correlation_results['correlation_tests'].items():\n",
    "        significance = \"***\" if test_results['p_value'] < 0.001 else \"**\" if test_results['p_value'] < 0.01 else \"*\" if test_results['p_value'] < 0.05 else \"ns\"\n",
    "        print(f\"{category}:\")\n",
    "        print(f\"  Mean luminosity: {test_results['mean_luminosity']:.3f}\")\n",
    "        print(f\"  Median luminosity: {test_results['median_luminosity']:.3f}\")\n",
    "        print(f\"  P-value: {test_results['p_value']:.4f} {significance}\")\n",
    "        print(f\"  Significant difference: {test_results['significant']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation visualization\n",
    "if 'viirs_luminosity' in integrated_gdf.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('POI-Luminosity Correlation Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Correlation heatmap\n",
    "    ax1 = axes[0, 0]\n",
    "    # Create dummy variables for categories\n",
    "    category_dummies = pd.get_dummies(integrated_gdf['category_group'])\n",
    "    corr_data = pd.concat([category_dummies, integrated_gdf[['viirs_luminosity']]], axis=1)\n",
    "    correlation_matrix = corr_data.corr()\n",
    "    \n",
    "    # Extract correlations with luminosity\n",
    "    lum_correlations = correlation_matrix['viirs_luminosity'].drop('viirs_luminosity')\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax1.imshow([[corr] for corr in lum_correlations.values], \n",
    "                   cmap='RdBu', vmin=-1, vmax=1, aspect='auto')\n",
    "    ax1.set_yticks(range(len(lum_correlations)))\n",
    "    ax1.set_yticklabels(lum_correlations.index, fontsize=10)\n",
    "    ax1.set_xticks([0])\n",
    "    ax1.set_xticklabels(['VIIRS Luminosity'])\n",
    "    ax1.set_title('Category-Luminosity Correlations')\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i, corr in enumerate(lum_correlations.values):\n",
    "        ax1.text(0, i, f'{corr:.3f}', ha='center', va='center', \n",
    "                fontweight='bold', color='white' if abs(corr) > 0.5 else 'black')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax1)\n",
    "    \n",
    "    # 2. Mean luminosity by category (bar plot)\n",
    "    ax2 = axes[0, 1]\n",
    "    mean_luminosity = integrated_gdf.groupby('category_group')['viirs_luminosity'].mean().sort_values(ascending=False)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(mean_luminosity)))\n",
    "    bars = ax2.bar(range(len(mean_luminosity)), mean_luminosity.values, color=colors)\n",
    "    ax2.set_xticks(range(len(mean_luminosity)))\n",
    "    ax2.set_xticklabels(mean_luminosity.index, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Mean VIIRS Luminosity')\n",
    "    ax2.set_title('Mean Luminosity by Category')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Violin plot - luminosity distribution by category\n",
    "    ax3 = axes[1, 0]\n",
    "    categories = integrated_gdf['category_group'].unique()\n",
    "    violin_data = [integrated_gdf[integrated_gdf['category_group'] == cat]['viirs_luminosity'].values \n",
    "                   for cat in categories]\n",
    "    \n",
    "    parts = ax3.violinplot(violin_data, positions=range(len(categories)), \n",
    "                          showmeans=True, showmedians=True)\n",
    "    \n",
    "    # Color the violin plots\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "    for i, pc in enumerate(parts['bodies']):\n",
    "        pc.set_facecolor(colors[i])\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    ax3.set_xticks(range(len(categories)))\n",
    "    ax3.set_xticklabels([cat[:10] + '...' if len(cat) > 10 else cat for cat in categories], \n",
    "                       rotation=45, ha='right')\n",
    "    ax3.set_ylabel('VIIRS Luminosity')\n",
    "    ax3.set_title('Luminosity Distribution by Category')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scatter plot - spatial correlation\n",
    "    ax4 = axes[1, 1]\n",
    "    scatter = ax4.scatter(integrated_gdf.geometry.x, integrated_gdf['viirs_luminosity'],\n",
    "                         c=[plt.cm.Set3(i/len(categories)) for i, cat in enumerate(categories) \n",
    "                            for _ in range(len(integrated_gdf[integrated_gdf['category_group'] == cat]))],\n",
    "                         s=30, alpha=0.7, edgecolors='white', linewidths=0.2)\n",
    "    ax4.set_xlabel('Longitude')\n",
    "    ax4.set_ylabel('VIIRS Luminosity')\n",
    "    ax4.set_title('Spatial vs Luminosity Correlation')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(integrated_gdf.geometry.x, integrated_gdf['viirs_luminosity'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax4.plot(integrated_gdf.geometry.x, p(integrated_gdf.geometry.x), \"r--\", alpha=0.8, linewidth=2)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    spatial_corr = integrated_gdf.geometry.x.corr(integrated_gdf['viirs_luminosity'])\n",
    "    ax4.text(0.05, 0.95, f'Correlation: {spatial_corr:.3f}', \n",
    "             transform=ax4.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Autocorrelation Analysis\n",
    "\n",
    "Let's examine spatial patterns and autocorrelation in the luminosity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform spatial autocorrelation analysis\n",
    "print(\"Performing spatial autocorrelation analysis...\")\n",
    "spatial_results = analyzer.spatial_autocorrelation_analysis(integrated_gdf)\n",
    "\n",
    "print(\"\\nSPATIAL AUTOCORRELATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'morans_i' in spatial_results and spatial_results['morans_i'] is not None:\n",
    "    morans_i = spatial_results['morans_i']\n",
    "    print(f\"\\nMoran's I Statistic: {morans_i:.4f}\")\n",
    "    \n",
    "    # Interpret Moran's I\n",
    "    if morans_i > 0.1:\n",
    "        interpretation = \"Strong positive spatial autocorrelation\"\n",
    "        meaning = \"Similar luminosity values tend to cluster together\"\n",
    "    elif morans_i > 0.05:\n",
    "        interpretation = \"Moderate positive spatial autocorrelation\"\n",
    "        meaning = \"Some clustering of similar luminosity values\"\n",
    "    elif morans_i > -0.05:\n",
    "        interpretation = \"No significant spatial autocorrelation\"\n",
    "        meaning = \"Luminosity values are randomly distributed\"\n",
    "    elif morans_i > -0.1:\n",
    "        interpretation = \"Moderate negative spatial autocorrelation\"\n",
    "        meaning = \"Dissimilar luminosity values tend to be neighbors\"\n",
    "    else:\n",
    "        interpretation = \"Strong negative spatial autocorrelation\"\n",
    "        meaning = \"Strong checkerboard pattern of luminosity values\"\n",
    "    \n",
    "    print(f\"Interpretation: {interpretation}\")\n",
    "    print(f\"Meaning: {meaning}\")\n",
    "    \n",
    "    if 'spatial_autocorrelation' in spatial_results:\n",
    "        print(f\"Pattern: {spatial_results['spatial_autocorrelation'].title()} autocorrelation\")\n",
    "    \n",
    "    if 'autocorrelation_strength' in spatial_results:\n",
    "        strength = spatial_results['autocorrelation_strength']\n",
    "        strength_desc = \"Strong\" if strength > 0.3 else \"Moderate\" if strength > 0.1 else \"Weak\"\n",
    "        print(f\"Strength: {strength_desc} ({strength:.4f})\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSpatial autocorrelation analysis could not be completed.\")\n",
    "    print(\"This might be due to insufficient data or computational constraints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spatial patterns\n",
    "if 'viirs_luminosity' in integrated_gdf.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Spatial Pattern Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Spatial distribution with luminosity\n",
    "    ax1 = axes[0, 0]\n",
    "    scatter = ax1.scatter(integrated_gdf.geometry.x, integrated_gdf.geometry.y,\n",
    "                         c=integrated_gdf['viirs_luminosity'], s=50, alpha=0.8,\n",
    "                         cmap='YlOrRd', edgecolors='black', linewidths=0.3)\n",
    "    plt.colorbar(scatter, ax=ax1, label='VIIRS Luminosity')\n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('Spatial Distribution of Luminosity')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Luminosity hotspots (high values)\n",
    "    ax2 = axes[0, 1]\n",
    "    # Define hotspots as top 20% luminosity values\n",
    "    hotspot_threshold = integrated_gdf['viirs_luminosity'].quantile(0.8)\n",
    "    hotspots = integrated_gdf[integrated_gdf['viirs_luminosity'] >= hotspot_threshold]\n",
    "    \n",
    "    ax2.scatter(integrated_gdf.geometry.x, integrated_gdf.geometry.y, \n",
    "               c='lightgray', s=20, alpha=0.5, label='All POIs')\n",
    "    ax2.scatter(hotspots.geometry.x, hotspots.geometry.y,\n",
    "               c='red', s=60, alpha=0.8, label=f'Hotspots (>{hotspot_threshold:.1f})')\n",
    "    ax2.set_xlabel('Longitude')\n",
    "    ax2.set_ylabel('Latitude')\n",
    "    ax2.set_title(f'Luminosity Hotspots ({len(hotspots)} POIs)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Spatial clustering by category\n",
    "    ax3 = axes[1, 0]\n",
    "    categories = integrated_gdf['category_group'].unique()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "    \n",
    "    for i, category in enumerate(categories):\n",
    "        cat_data = integrated_gdf[integrated_gdf['category_group'] == category]\n",
    "        ax3.scatter(cat_data.geometry.x, cat_data.geometry.y,\n",
    "                   c=[colors[i]], s=40, alpha=0.7, label=category)\n",
    "    \n",
    "    ax3.set_xlabel('Longitude')\n",
    "    ax3.set_ylabel('Latitude')\n",
    "    ax3.set_title('Spatial Distribution by Category')\n",
    "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Distance vs luminosity relationship\n",
    "    ax4 = axes[1, 1]\n",
    "    # Calculate distance from center\n",
    "    center_x = integrated_gdf.geometry.x.mean()\n",
    "    center_y = integrated_gdf.geometry.y.mean()\n",
    "    \n",
    "    distances = np.sqrt((integrated_gdf.geometry.x - center_x)**2 + \n",
    "                       (integrated_gdf.geometry.y - center_y)**2)\n",
    "    \n",
    "    scatter = ax4.scatter(distances, integrated_gdf['viirs_luminosity'],\n",
    "                         c=integrated_gdf['viirs_luminosity'], s=30, alpha=0.7,\n",
    "                         cmap='viridis', edgecolors='white', linewidths=0.2)\n",
    "    ax4.set_xlabel('Distance from Center (degrees)')\n",
    "    ax4.set_ylabel('VIIRS Luminosity')\n",
    "    ax4.set_title('Distance vs Luminosity')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(distances, integrated_gdf['viirs_luminosity'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax4.plot(distances, p(distances), \"r--\", alpha=0.8, linewidth=2)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    dist_corr = distances.corr(integrated_gdf['viirs_luminosity'])\n",
    "    ax4.text(0.05, 0.95, f'Correlation: {dist_corr:.3f}', \n",
    "             transform=ax4.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print spatial pattern insights\n",
    "    print(f\"\\nSpatial Pattern Insights:\")\n",
    "    print(f\"  - Identified {len(hotspots)} luminosity hotspots (top 20%)\")\n",
    "    print(f\"  - Hotspot threshold: {hotspot_threshold:.2f} luminosity units\")\n",
    "    print(f\"  - Distance-luminosity correlation: {dist_corr:.3f}\")\n",
    "    \n",
    "    if abs(dist_corr) > 0.3:\n",
    "        pattern = \"decreases\" if dist_corr < 0 else \"increases\"\n",
    "        print(f\"  - Luminosity {pattern} with distance from center (moderate to strong correlation)\")\n",
    "    else:\n",
    "        print(f\"  - No strong distance-luminosity relationship detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cluster Analysis Evaluation\n",
    "\n",
    "Let's evaluate the quality and characteristics of our clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cluster analysis\n",
    "print(\"Evaluating cluster analysis results...\")\n",
    "cluster_results = analyzer.cluster_analysis(integrated_gdf)\n",
    "\n",
    "print(\"\\nCLUSTER ANALYSIS RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'cluster_summary' in cluster_results:\n",
    "    print(\"\\nCluster Summary Statistics:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    cluster_summary = cluster_results['cluster_summary']\n",
    "    for cluster_id in sorted(cluster_summary.keys()):\n",
    "        cluster_stats = cluster_summary[cluster_id]\n",
    "        print(f\"\\nCluster {cluster_id}:\")\n",
    "        \n",
    "        if 'viirs_luminosity' in cluster_stats:\n",
    "            lum_stats = cluster_stats['viirs_luminosity']\n",
    "            print(f\"  POI Count: {lum_stats['count']}\")\n",
    "            print(f\"  Mean Luminosity: {lum_stats['mean']:.3f}\")\n",
    "            print(f\"  Std Luminosity: {lum_stats['std']:.3f}\")\n",
    "            print(f\"  Median Luminosity: {lum_stats['median']:.3f}\")\n",
    "        \n",
    "        if 'category_group' in cluster_stats:\n",
    "            dominant_category = cluster_stats['category_group']\n",
    "            print(f\"  Dominant Category: {dominant_category}\")\n",
    "\n",
    "# Silhouette score evaluation\n",
    "if 'silhouette_score' in cluster_results:\n",
    "    silhouette_score = cluster_results['silhouette_score']\n",
    "    cluster_quality = cluster_results.get('cluster_quality', 'unknown')\n",
    "    \n",
    "    print(f\"\\nCluster Quality Assessment:\")\n",
    "    print(f\"  Silhouette Score: {silhouette_score:.4f}\")\n",
    "    print(f\"  Quality Rating: {cluster_quality.title()}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if silhouette_score > 0.7:\n",
    "        interpretation = \"Excellent clustering - clusters are well-separated and cohesive\"\n",
    "    elif silhouette_score > 0.5:\n",
    "        interpretation = \"Good clustering - reasonable structure with some overlap\"\n",
    "    elif silhouette_score > 0.3:\n",
    "        interpretation = \"Moderate clustering - clusters exist but with significant overlap\"\n",
    "    elif silhouette_score > 0:\n",
    "        interpretation = \"Weak clustering - minimal cluster structure\"\n",
    "    else:\n",
    "        interpretation = \"Poor clustering - data points may be better assigned to other clusters\"\n",
    "    \n",
    "    print(f\"  Interpretation: {interpretation}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nCluster quality assessment could not be performed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster analysis\n",
    "if 'cluster' in integrated_gdf.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Cluster Analysis Evaluation', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    n_clusters = integrated_gdf['cluster'].nunique()\n",
    "    cluster_colors = plt.cm.tab10(np.linspace(0, 1, n_clusters))\n",
    "    \n",
    "    # 1. Spatial distribution of clusters\n",
    "    ax1 = axes[0, 0]\n",
    "    for i, cluster_id in enumerate(sorted(integrated_gdf['cluster'].unique())):\n",
    "        cluster_data = integrated_gdf[integrated_gdf['cluster'] == cluster_id]\n",
    "        ax1.scatter(cluster_data.geometry.x, cluster_data.geometry.y,\n",
    "                   c=[cluster_colors[i]], s=50, alpha=0.7, \n",
    "                   label=f'Cluster {cluster_id} ({len(cluster_data)} POIs)',\n",
    "                   edgecolors='white', linewidths=0.5)\n",
    "    \n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('Spatial Distribution of Clusters')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Cluster characteristics (if luminosity available)\n",
    "    ax2 = axes[0, 1]\n",
    "    if 'viirs_luminosity' in integrated_gdf.columns:\n",
    "        cluster_means = integrated_gdf.groupby('cluster')['viirs_luminosity'].mean()\n",
    "        cluster_stds = integrated_gdf.groupby('cluster')['viirs_luminosity'].std()\n",
    "        \n",
    "        bars = ax2.bar(cluster_means.index, cluster_means.values, \n",
    "                      yerr=cluster_stds.values, capsize=5,\n",
    "                      color=cluster_colors, alpha=0.7)\n",
    "        ax2.set_xlabel('Cluster ID')\n",
    "        ax2.set_ylabel('Mean VIIRS Luminosity')\n",
    "        ax2.set_title('Mean Luminosity by Cluster')\n",
    "        ax2.set_xticks(cluster_means.index)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + cluster_stds.iloc[i] + 0.1,\n",
    "                    f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Cluster size distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    cluster_sizes = integrated_gdf['cluster'].value_counts().sort_index()\n",
    "    bars = ax3.bar(cluster_sizes.index, cluster_sizes.values, \n",
    "                   color=cluster_colors, alpha=0.7)\n",
    "    ax3.set_xlabel('Cluster ID')\n",
    "    ax3.set_ylabel('Number of POIs')\n",
    "    ax3.set_title('Cluster Size Distribution')\n",
    "    ax3.set_xticks(cluster_sizes.index)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Cluster composition by category\n",
    "    ax4 = axes[1, 1]\n",
    "    cluster_category_crosstab = pd.crosstab(integrated_gdf['cluster'], \n",
    "                                           integrated_gdf['category_group'], \n",
    "                                           normalize='index') * 100\n",
    "    \n",
    "    cluster_category_crosstab.plot(kind='bar', ax=ax4, stacked=True, \n",
    "                                  colormap='Set3', alpha=0.8)\n",
    "    ax4.set_xlabel('Cluster ID')\n",
    "    ax4.set_ylabel('Percentage of POIs')\n",
    "    ax4.set_title('Cluster Composition by Category')\n",
    "    ax4.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax4.set_xticklabels(ax4.get_xticklabels(), rotation=0)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print cluster insights\n",
    "    print(f\"\\nCluster Analysis Insights:\")\n",
    "    print(f\"  - Total clusters identified: {n_clusters}\")\n",
    "    print(f\"  - Cluster sizes range from {cluster_sizes.min()} to {cluster_sizes.max()} POIs\")\n",
    "    \n",
    "    if 'viirs_luminosity' in integrated_gdf.columns:\n",
    "        highest_lum_cluster = cluster_means.idxmax()\n",
    "        lowest_lum_cluster = cluster_means.idxmin()\n",
    "        print(f\"  - Highest luminosity cluster: {highest_lum_cluster} (mean: {cluster_means[highest_lum_cluster]:.2f})\")\n",
    "        print(f\"  - Lowest luminosity cluster: {lowest_lum_cluster} (mean: {cluster_means[lowest_lum_cluster]:.2f})\")\n",
    "    \n",
    "    # Find most homogeneous cluster (by category)\n",
    "    cluster_category_max = cluster_category_crosstab.max(axis=1)\n",
    "    most_homogeneous_cluster = cluster_category_max.idxmax()\n",
    "    homogeneity_score = cluster_category_max[most_homogeneous_cluster]\n",
    "    print(f\"  - Most homogeneous cluster: {most_homogeneous_cluster} ({homogeneity_score:.1f}% single category)\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo cluster data available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Anomaly Analysis\n",
    "\n",
    "Let's examine the anomalous patterns we've identified in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform anomaly analysis\n",
    "print(\"Analyzing anomalous POI-luminosity patterns...\")\n",
    "anomaly_results = analyzer.anomaly_analysis(integrated_gdf)\n",
    "\n",
    "print(\"\\nANOMALY ANALYSIS RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'anomaly_distribution' in anomaly_results:\n",
    "    print(\"\\nAnomaly Type Distribution:\")\n",
    "    anomaly_dist = anomaly_results['anomaly_distribution']\n",
    "    total_pois = sum(anomaly_dist.values())\n",
    "    \n",
    "    for anomaly_type, count in anomaly_dist.items():\n",
    "        percentage = (count / total_pois) * 100\n",
    "        print(f\"  {anomaly_type}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "if 'anomaly_characteristics' in anomaly_results:\n",
    "    print(\"\\nAnomaly Characteristics:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    anomaly_chars = anomaly_results['anomaly_characteristics']\n",
    "    for anomaly_type, characteristics in anomaly_chars.items():\n",
    "        print(f\"\\n{anomaly_type}:\")\n",
    "        print(f\"  Count: {characteristics['count']:,}\")\n",
    "        \n",
    "        if 'mean_luminosity' in characteristics:\n",
    "            print(f\"  Mean Luminosity: {characteristics['mean_luminosity']:.3f}\")\n",
    "            print(f\"  Median Luminosity: {characteristics['median_luminosity']:.3f}\")\n",
    "        \n",
    "        if 'dominant_category' in characteristics and characteristics['dominant_category']:\n",
    "            print(f\"  Dominant Category: {characteristics['dominant_category']}\")\n",
    "\n",
    "# Provide interpretations\n",
    "print(\"\\nAnomaly Interpretations:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "if 'anomaly_distribution' in anomaly_results:\n",
    "    for anomaly_type in anomaly_results['anomaly_distribution'].keys():\n",
    "        if anomaly_type == 'Normal':\n",
    "            print(f\"\\n{anomaly_type}: POIs with expected luminosity levels for their category\")\n",
    "        elif anomaly_type == 'High Light, Non-Commercial':\n",
    "            print(f\"\\n{anomaly_type}: Areas with high luminosity but non-commercial POIs\")\n",
    "            print(f\"  → Possible interpretations:\")\n",
    "            print(f\"    • Residential areas with good lighting infrastructure\")\n",
    "            print(f\"    • Essential services in well-lit areas\")\n",
    "            print(f\"    • Mixed-use areas with bright street lighting\")\n",
    "        elif anomaly_type == 'Low Light, Commercial':\n",
    "            print(f\"\\n{anomaly_type}: Commercial POIs in areas with low luminosity\")\n",
    "            print(f\"  → Possible interpretations:\")\n",
    "            print(f\"    • Underlit commercial areas needing infrastructure improvement\")\n",
    "            print(f\"    • Small-scale or informal commercial activities\")\n",
    "            print(f\"    • Areas with potential safety or accessibility concerns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Comprehensive Analysis Report\n",
    "\n",
    "Finally, let's generate a comprehensive report with all our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive analysis report\n",
    "print(\"Generating comprehensive exploratory analysis report...\")\n",
    "comprehensive_report = analyzer.generate_analysis_report(integrated_gdf)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPREHENSIVE EXPLORATORY ANALYSIS COMPLETED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display key findings summary\n",
    "print(\"\\nKEY FINDINGS SUMMARY:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Basic statistics\n",
    "if 'descriptive_stats' in comprehensive_report:\n",
    "    stats = comprehensive_report['descriptive_stats']\n",
    "    print(f\"\\n📊 Dataset Overview:\")\n",
    "    print(f\"   • Total POIs analyzed: {stats['total_pois']:,}\")\n",
    "    print(f\"   • Categories represented: {stats['unique_categories']}\")\n",
    "    \n",
    "    if 'luminosity_stats' in stats:\n",
    "        lum_stats = stats['luminosity_stats']\n",
    "        print(f\"   • Luminosity range: {lum_stats['min']:.2f} to {lum_stats['max']:.2f}\")\n",
    "        print(f\"   • Mean luminosity: {lum_stats['mean']:.2f}\")\n",
    "\n",
    "# Correlation findings\n",
    "if 'correlation_analysis' in comprehensive_report:\n",
    "    corr_analysis = comprehensive_report['correlation_analysis']\n",
    "    if 'category_luminosity_correlations' in corr_analysis:\n",
    "        correlations = corr_analysis['category_luminosity_correlations']\n",
    "        print(f\"\\n🔗 Correlation Findings:\")\n",
    "        \n",
    "        # Find strongest correlations\n",
    "        sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        if len(sorted_corr) > 0:\n",
    "            strongest_cat, strongest_corr = sorted_corr[0]\n",
    "            print(f\"   • Strongest correlation: {strongest_cat} ({strongest_corr:.3f})\")\n",
    "        \n",
    "        # Identify significant correlations\n",
    "        significant_corr = [(cat, corr) for cat, corr in correlations.items() if abs(corr) > 0.3]\n",
    "        print(f\"   • Categories with significant correlations: {len(significant_corr)}\")\n",
    "\n",
    "# Spatial pattern insights\n",
    "if 'spatial_analysis' in comprehensive_report:\n",
    "    spatial_analysis = comprehensive_report['spatial_analysis']\n",
    "    if 'morans_i' in spatial_analysis and spatial_analysis['morans_i'] is not None:\n",
    "        morans_i = spatial_analysis['morans_i']\n",
    "        print(f\"\\n🗺️ Spatial Pattern Insights:\")\n",
    "        print(f\"   • Moran's I statistic: {morans_i:.4f}\")\n",
    "        print(f\"   • Spatial pattern: {spatial_analysis.get('spatial_autocorrelation', 'Unknown').title()}\")\n",
    "        \n",
    "        if abs(morans_i) > 0.1:\n",
    "            strength = \"Strong\" if abs(morans_i) > 0.3 else \"Moderate\"\n",
    "            direction = \"clustering\" if morans_i > 0 else \"dispersion\"\n",
    "            print(f\"   • Pattern strength: {strength} {direction}\")\n",
    "\n",
    "# Cluster quality\n",
    "if 'cluster_analysis' in comprehensive_report:\n",
    "    cluster_analysis = comprehensive_report['cluster_analysis']\n",
    "    if 'silhouette_score' in cluster_analysis:\n",
    "        silhouette = cluster_analysis['silhouette_score']\n",
    "        quality = cluster_analysis['cluster_quality']\n",
    "        print(f\"\\n🎯 Clustering Results:\")\n",
    "        print(f\"   • Cluster quality: {quality.title()}\")\n",
    "        print(f\"   • Silhouette score: {silhouette:.3f}\")\n",
    "\n",
    "# Anomaly insights\n",
    "if 'anomaly_analysis' in comprehensive_report:\n",
    "    anomaly_analysis = comprehensive_report['anomaly_analysis']\n",
    "    if 'anomaly_distribution' in anomaly_analysis:\n",
    "        anomalies = anomaly_analysis['anomaly_distribution']\n",
    "        print(f\"\\n⚠️ Anomaly Detection:\")\n",
    "        \n",
    "        total_anomalies = sum([count for atype, count in anomalies.items() if atype != 'Normal'])\n",
    "        total_pois = sum(anomalies.values())\n",
    "        anomaly_rate = (total_anomalies / total_pois) * 100\n",
    "        \n",
    "        print(f\"   • Anomalous POIs: {total_anomalies:,} ({anomaly_rate:.1f}%)\")\n",
    "        \n",
    "        # Highlight specific anomaly types\n",
    "        for atype, count in anomalies.items():\n",
    "            if atype != 'Normal' and count > 0:\n",
    "                percentage = (count / total_pois) * 100\n",
    "                print(f\"   • {atype}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS OUTPUTS:\")\n",
    "print(f\"📁 Results directory: {analyzer.output_dir}\")\n",
    "print(f\"📊 Analysis report: {analyzer.output_dir}/exploratory_analysis_report.json\")\n",
    "print(f\"📈 Summary plots: {analyzer.output_dir}/plots/exploratory_analysis_summary.png\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review detailed analysis report (JSON file)\")\n",
    "print(\"2. Run notebook 04_visualization.ipynb for interactive visualizations\")\n",
    "print(\"3. Use findings to inform urban planning decisions\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a final comprehensive summary table\n",
    "summary_data = {\n",
    "    'Metric': [],\n",
    "    'Value': [],\n",
    "    'Interpretation': []\n",
    "}\n",
    "\n",
    "# Add key metrics\n",
    "if 'descriptive_stats' in comprehensive_report:\n",
    "    stats = comprehensive_report['descriptive_stats']\n",
    "    \n",
    "    summary_data['Metric'].extend([\n",
    "        'Total POIs',\n",
    "        'Categories',\n",
    "        'Mean Luminosity',\n",
    "        'Study Area'\n",
    "    ])\n",
    "    \n",
    "    summary_data['Value'].extend([\n",
    "        f\"{stats['total_pois']:,}\",\n",
    "        str(stats['unique_categories']),\n",
    "        f\"{stats.get('luminosity_stats', {}).get('mean', 0):.2f}\",\n",
    "        f\"{stats.get('spatial_extent', {}).get('width_km', 0):.1f} × {stats.get('spatial_extent', {}).get('height_km', 0):.1f} km\"\n",
    "    ])\n",
    "    \n",
    "    summary_data['Interpretation'].extend([\n",
    "        'Comprehensive POI coverage',\n",
    "        'Diverse urban functions represented',\n",
    "        'Average nighttime illumination level',\n",
    "        'Focused neighborhood-scale analysis'\n",
    "    ])\n",
    "\n",
    "# Add correlation insights\n",
    "if 'correlation_analysis' in comprehensive_report:\n",
    "    corr_analysis = comprehensive_report['correlation_analysis']\n",
    "    if 'category_luminosity_correlations' in corr_analysis:\n",
    "        correlations = corr_analysis['category_luminosity_correlations']\n",
    "        strongest_corr = max(correlations.items(), key=lambda x: abs(x[1]))\n",
    "        \n",
    "        summary_data['Metric'].append('Strongest Category Correlation')\n",
    "        summary_data['Value'].append(f\"{strongest_corr[0]}: {strongest_corr[1]:.3f}\")\n",
    "        \n",
    "        if abs(strongest_corr[1]) > 0.5:\n",
    "            interpretation = \"Strong luminosity association\"\n",
    "        elif abs(strongest_corr[1]) > 0.3:\n",
    "            interpretation = \"Moderate luminosity association\"\n",
    "        else:\n",
    "            interpretation = \"Weak luminosity association\"\n",
    "        \n",
    "        summary_data['Interpretation'].append(interpretation)\n",
    "\n",
    "# Add clustering results\n",
    "if 'cluster_analysis' in comprehensive_report and 'silhouette_score' in comprehensive_report['cluster_analysis']:\n",
    "    silhouette = comprehensive_report['cluster_analysis']['silhouette_score']\n",
    "    quality = comprehensive_report['cluster_analysis']['cluster_quality']\n",
    "    \n",
    "    summary_data['Metric'].append('Clustering Quality')\n",
    "    summary_data['Value'].append(f\"{quality.title()} ({silhouette:.3f})\")\n",
    "    summary_data['Interpretation'].append('Spatial clustering effectiveness')\n",
    "\n",
    "# Add anomaly rate\n",
    "if 'anomaly_analysis' in comprehensive_report and 'anomaly_distribution' in comprehensive_report['anomaly_analysis']:\n",
    "    anomalies = comprehensive_report['anomaly_analysis']['anomaly_distribution']\n",
    "    total_anomalies = sum([count for atype, count in anomalies.items() if atype != 'Normal'])\n",
    "    total_pois = sum(anomalies.values())\n",
    "    anomaly_rate = (total_anomalies / total_pois) * 100\n",
    "    \n",
    "    summary_data['Metric'].append('Anomaly Rate')\n",
    "    summary_data['Value'].append(f\"{anomaly_rate:.1f}%\")\n",
    "    \n",
    "    if anomaly_rate > 20:\n",
    "        interpretation = \"High anomaly rate - diverse patterns\"\n",
    "    elif anomaly_rate > 10:\n",
    "        interpretation = \"Moderate anomaly rate - some outliers\"\n",
    "    else:\n",
    "        interpretation = \"Low anomaly rate - consistent patterns\"\n",
    "    \n",
    "    summary_data['Interpretation'].append(interpretation)\n",
    "\n",
    "# Create and display summary table\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\nFINAL ANALYSIS SUMMARY TABLE\")\n",
    "print(\"=\" * 80)\n",
    "display(summary_df)\n",
    "\n",
    "# Save summary table\n",
    "summary_df.to_csv(analyzer.output_dir / 'exploratory_analysis_summary.csv', index=False)\n",
    "print(f\"\\nSummary table saved to: {analyzer.output_dir}/exploratory_analysis_summary.csv\")\n",
    "\n",
    "print(\"\\n🎉 EXPLORATORY DATA ANALYSIS COMPLETE! 🎉\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}